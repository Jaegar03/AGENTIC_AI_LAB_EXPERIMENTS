{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI + CS Interview Assistant: Local Fine-Tuning with LLaMA 3.2\n",
    "\n",
    "**Goal**: Fine-tune a LLaMA 3.2 model locally to act as an expert technical interview assistant.\n",
    "\n",
    "**Overview**:\n",
    "1. **Setup**: Compare with local Ollama model.\n",
    "2. **Data**: Load local interview dataset.\n",
    "3. **Train**: Use QLoRA (4-bit quantization) for efficient local training.\n",
    "4. **Verify**: Compare responses before and after training.\n",
    "\n",
    "**Note**: This notebook assumes you have a GPU (NVIDIA) for QLoRA. If running strictly on CPU, training will be significantly slower and QLoRA (bitsandbytes) might need specific configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "We need `transformers`, `peft` for adapters, `bitsandbytes` for quantization, and `ollama` for the baseline comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install -q transformers datasets peft bitsandbytes ollama trl accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import ollama\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Suppress excessive warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Local Dataset\n",
    "We load the `ai_cs_interview_120.json` file. Ensure this file is in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"ai_cs_interview_120.json\"\n",
    "\n",
    "try:\n",
    "    # Load dataset from local JSON\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")\n",
    "    print(f\"Dataset Loaded. Size: {len(dataset)} samples\")\n",
    "    print(\"Sample Entry:\", dataset[0])\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please ensure 'ai_cs_interview_120.json' exists in the notebook directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Inference (Before Training)\n",
    "We use the locally running Ollama instance to check how the base LLaMA 3.2 model answers interview questions *without* fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(prompt, model=\"llama3.2\"):\n",
    "    try:\n",
    "        response = ollama.chat(model=model, messages=[\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ])\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Ollama Error: {str(e)}\"\n",
    "\n",
    "# Test Prompts\n",
    "test_instruction = \"Explain the difference between a Process and a Thread in an OS context.\"\n",
    "\n",
    "print(\"--- Baseline (Ollama) ---\")\n",
    "baseline_response = query_ollama(test_instruction)\n",
    "print(f\"Instruction: {test_instruction}\\n\")\n",
    "print(f\"Response:\\n{baseline_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning Setup (QLoRA)\n",
    "\n",
    "We will fine-tune usage Hugging Face Transformers. \n",
    "**Important**: Ollama stores models in GGUF format which isn't directly trainable by standard tools. We will download the base weights for `Llama-3.2-1B-Instruct` (or 3B) from Hugging Face to perform the training, then save the adapter.\n",
    "\n",
    "*Note: You need a Hugging Face token if accessing gated models, though Llama 3.2 1B/3B open weights are often available.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ID - We use 1B or 3B for local efficiency. \n",
    "# Ensure you have accepted the license on HF Hub if using meta-llama/Llama-3.2-1B-Instruct\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# Or use \"unsloth/Llama-3.2-1B-Instruct\" for faster download if available\n",
    "\n",
    "NEW_MODEL_NAME = \"llama-3.2-interview-assistant\"\n",
    "\n",
    "# QLoRA Configuration (4-bit quantization)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load Base Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the Dataset\n",
    "We convert the (Instruction, Input, Output) format into a single prompt string for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(sample):\n",
    "    # Standard Alpaca/Instruction format\n",
    "    if sample.get(\"input\"):\n",
    "        text = f\"### Instruction:\\n{sample['instruction']}\\n\\n### Input:\\n{sample['input']}\\n\\n### Response:\\n{sample['output']}\"\n",
    "    else:\n",
    "        text = f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Configuration\n",
    "We configure the Low-Rank Adapter (LoRA) to train only a small percentage of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "We use the `SFTTrainer` (Supervised Fine-tuning Trainer) from `trl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,          # Quick epoch for demonstration\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned adapter\n",
    "trainer.model.save_pretrained(NEW_MODEL_NAME)\n",
    "tokenizer.save_pretrained(NEW_MODEL_NAME)\n",
    "print(f\"Model adapter saved to locally at: {NEW_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Comparison (After Training)\n",
    "We now reload the model with the trained LoRA adapter to see the difference.\n",
    "\n",
    "*Note: To run this in Ollama (outside Python), you would typically fuse this adapter with the base model and convert it to GGUF format using `llama.cpp`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load base model again (or reuse if memory allows, simpler to reload for clean state)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load the adapter we just trained\n",
    "ft_model = PeftModel.from_pretrained(base_model, NEW_MODEL_NAME)\n",
    "ft_model = ft_model.merge_and_unload() # Merge for faster inference\n",
    "\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "ft_tokenizer.pad_token = ft_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_finetuned(instruction, input_text=\"\"):\n",
    "    # Format prompt exactly as in training\n",
    "    if input_text:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "        \n",
    "    inputs = ft_tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    outputs = ft_model.generate(**inputs, max_new_tokens=200, use_cache=True)\n",
    "    response = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the response part if possible\n",
    "    if \"### Response:\" in response:\n",
    "        return response.split(\"### Response:\")[1].strip()\n",
    "    return response\n",
    "\n",
    "print(\"--- Fine-Tuned Model Inference ---\")\n",
    "ft_response = query_finetuned(test_instruction)\n",
    "print(f\"Instruction: {test_instruction}\\n\")\n",
    "print(f\"Response:\\n{ft_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Comparison\n",
    "Side-by-side view of the base Ollama model vs the Fine-Tuned Local model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== COMPARISON ===\\n\")\n",
    "print(f\"PROMPT: {test_instruction}\\n\")\n",
    "\n",
    "print(\"[BEFORE - Ollama Base Model]\")\n",
    "print(baseline_response)\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "print(\"[AFTER - Fine-Tuned Adapter]\")\n",
    "print(ft_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
