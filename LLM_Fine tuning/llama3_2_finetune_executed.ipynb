{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AI + CS Interview Assistant: Local Fine-Tuning with LLaMA 3.2\n",
                "\n",
                "**Goal**: Fine-tune a LLaMA 3.2 model locally to act as an expert technical interview assistant.\n",
                "\n",
                "**Overview**:\n",
                "1. **Setup**: Compare with local Ollama model.\n",
                "2. **Data**: Load local interview dataset.\n",
                "3. **Train**: Use QLoRA (4-bit quantization) for efficient local training.\n",
                "4. **Verify**: Compare responses before and after training.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup\n",
                "We need `transformers`, `peft` for adapters, `bitsandbytes` for quantization, and `ollama` for baseline comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
                        "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
                        "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
                        "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.41.1)\n",
                        "Requirement already satisfied: ollama in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
                        "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
                        "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n"
                    ]
                }
            ],
            "source": [
                "# Install necessary libraries\n",
                "!pip install -q transformers datasets peft bitsandbytes ollama trl accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import ollama\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments,\n",
                "    pipeline\n",
                ")\n",
                "from peft import LoraConfig, PeftModel\n",
                "from trl import SFTTrainer\n",
                "\n",
                "# Suppress excessive warnings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Local Dataset\n",
                "We load the `ai_cs_interview_120.json` file. Ensure this file is in the same directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset Loaded. Size: 120 samples\n",
                        "Sample Entry: {'instruction': 'Explain the concept of Overfitting.', 'input': '', 'output': 'Overfitting occurs when a statistical model or machine learning algorithm captures the noise of the data rather than the underlying pattern.'}\n"
                    ]
                }
            ],
            "source": [
                "dataset_file = \"ai_cs_interview_120.json\"\n",
                "\n",
                "try:\n",
                "    # Load dataset from local JSON\n",
                "    dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")\n",
                "    print(f\"Dataset Loaded. Size: {len(dataset)} samples\")\n",
                "    print(\"Sample Entry:\", dataset[0])\n",
                "except Exception as e:\n",
                "    print(f\"Error loading dataset: {e}\")\n",
                "    print(\"Please ensure 'ai_cs_interview_120.json' exists in the notebook directory.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Baseline Inference (Before Training)\n",
                "We use the locally running Ollama instance to check how the base LLaMA 3.2 model answers interview questions *without* fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Baseline (Ollama) ---\n",
                        "Instruction: Explain the difference between a Process and a Thread in an OS context.\n",
                        "\n",
                        "Response:\n",
                        "A process and a thread are both units of execution in an operating system, but they differ in several key ways.\n",
                        "\n",
                        "**Process:**\n",
                        "A process is an independent program in execution. It has its own memory space, stack, and resources. Processes are isolated from each other, meaning one process cannot directly access the memory of another.\n",
                        "\n",
                        "**Thread:**\n",
                        "A thread is a lightweight unit of execution within a process. Threads share the same memory space and resources of the parent process. This makes communication between threads faster but also requires careful synchronization.\n"
                    ]
                }
            ],
            "source": [
                "def query_ollama(prompt, model=\"llama3.2\"):\n",
                "    try:\n",
                "        response = ollama.chat(model=model, messages=[\n",
                "            {'role': 'user', 'content': prompt}\n",
                "        ])\n",
                "        return response['message']['content']\n",
                "    except Exception as e:\n",
                "        return f\"Ollama Error: {str(e)}\"\n",
                "\n",
                "# Test Prompts\n",
                "test_instruction = \"Explain the difference between a Process and a Thread in an OS context.\"\n",
                "\n",
                "print(\"--- Baseline (Ollama) ---\")\n",
                "baseline_response = query_ollama(test_instruction)\n",
                "print(f\"Instruction: {test_instruction}\\n\")\n",
                "print(f\"Response:\\n{baseline_response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Fine-Tuning Setup (QLoRA)\n",
                "\n",
                "We will fine-tune usage Hugging Face Transformers. \n",
                "**Important**: Ollama stores models in GGUF format which isn't directly trainable by standard tools. We will download the base weights for `Llama-3.2-1B-Instruct` (or 3B) from Hugging Face to perform the training, then save the adapter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]\n"
                    ]
                }
            ],
            "source": [
                "# Model ID - We use 1B or 3B for local efficiency. \n",
                "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
                "\n",
                "NEW_MODEL_NAME = \"llama-3.2-interview-assistant\"\n",
                "\n",
                "# QLoRA Configuration (4-bit quantization)\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=False,\n",
                ")\n",
                "\n",
                "# Load Base Model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "model.config.use_cache = False\n",
                "model.config.pretraining_tp = 1\n",
                "\n",
                "# Load Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Formatting the Dataset\n",
                "We convert the (Instruction, Input, Output) format into a single prompt string for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_prompt(sample):\n",
                "    # Standard Alpaca/Instruction format\n",
                "    if sample.get(\"input\"):\n",
                "        text = f\"### Instruction:\\n{sample['instruction']}\\n\\n### Input:\\n{sample['input']}\\n\\n### Response:\\n{sample['output']}\"\n",
                "    else:\n",
                "        text = f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}\"\n",
                "    return {\"text\": text}\n",
                "\n",
                "dataset = dataset.map(format_prompt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "peft_config = LoraConfig(\n",
                "    lora_alpha=16,\n",
                "    lora_dropout=0.1,\n",
                "    r=64,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training\n",
                "We use the `SFTTrainer` (Supervised Fine-tuning Trainer) from `trl`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Map: 100%|██████████| 120/120 [00:00<00:00, 568.12 examples/s]\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [30/30 02:45, Epoch 1/1]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>10</td>\n",
                            "      <td>1.802400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>20</td>\n",
                            "      <td>1.154200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>30</td>\n",
                            "      <td>0.895100</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    num_train_epochs=1,          # Quick epoch for demonstration\n",
                "    per_device_train_batch_size=4,\n",
                "    gradient_accumulation_steps=1,\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    save_steps=25,\n",
                "    logging_steps=10,\n",
                "    learning_rate=2e-4,\n",
                "    weight_decay=0.001,\n",
                "    fp16=True if torch.cuda.is_available() else False,\n",
                "    bf16=False,\n",
                "    max_grad_norm=0.3,\n",
                "    max_steps=-1,\n",
                "    warmup_ratio=0.03,\n",
                "    group_by_length=True,\n",
                "    lr_scheduler_type=\"constant\",\n",
                "    report_to=\"none\"\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=peft_config,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=None,\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "# Start training\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model adapter saved to locally at: llama-3.2-interview-assistant\n"
                    ]
                }
            ],
            "source": [
                "# Save the fine-tuned adapter\n",
                "trainer.model.save_pretrained(NEW_MODEL_NAME)\n",
                "tokenizer.save_pretrained(NEW_MODEL_NAME)\n",
                "print(f\"Model adapter saved to locally at: {NEW_MODEL_NAME}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference Comparison (After Training)\n",
                "We now reload the model with the trained LoRA adapter to see the difference.\n",
                "\n",
                "*Note: To run this in Ollama (outside Python), you would typically fuse this adapter with the base model and convert it to GGUF format using `llama.cpp`.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n"
                    ]
                }
            ],
            "source": [
                "from peft import PeftModel\n",
                "\n",
                "# Load base model again (or reuse if memory allows, simpler to reload for clean state)\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    low_cpu_mem_usage=True,\n",
                "    return_dict=True,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\",\n",
                ")\n",
                "\n",
                "# Load the adapter we just trained\n",
                "ft_model = PeftModel.from_pretrained(base_model, NEW_MODEL_NAME)\n",
                "ft_model = ft_model.merge_and_unload() # Merge for faster inference\n",
                "\n",
                "ft_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "ft_tokenizer.pad_token = ft_tokenizer.eos_token"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Fine-Tuned Model Inference ---\n",
                        "Instruction: Explain the difference between a Process and a Thread in an OS context.\n",
                        "\n",
                        "Response:\n",
                        "In the context of Operating Systems, a Process is a heavyweight unit of execution that has its own separate memory address space, including text, data, and stack segments. Processes are isolated from one another.\n",
                        "\n",
                        "A Thread, often called a lightweight process, exists within a process. Threads share the same address space (code and data) but have their own stack and registers. Context switching between threads is faster than between processes because improved locality and less overhead are required to switch."
                    ]
                }
            ],
            "source": [
                "def query_finetuned(instruction, input_text=\"\"):\n",
                "    # Format prompt exactly as in training\n",
                "    if input_text:\n",
                "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
                "    else:\n",
                "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
                "        \n",
                "    inputs = ft_tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
                "    outputs = ft_model.generate(**inputs, max_new_tokens=200, use_cache=True)\n",
                "    response = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    # Extract just the response part if possible\n",
                "    if \"### Response:\" in response:\n",
                "        return response.split(\"### Response:\")[1].strip()\n",
                "    return response\n",
                "\n",
                "print(\"--- Fine-Tuned Model Inference ---\")\n",
                "ft_response = query_finetuned(test_instruction)\n",
                "print(f\"Instruction: {test_instruction}\\n\")\n",
                "print(f\"Response:\\n{ft_response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Results Comparison\n",
                "Side-by-side view of the base Ollama model vs the Fine-Tuned Local model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== COMPARISON ===\n",
                        "\n",
                        "PROMPT: Explain the difference between a Process and a Thread in an OS context.\n",
                        "\n",
                        "[BEFORE - Ollama Base Model]\n",
                        "A process and a thread are both units of execution in an operating system, but they differ in several key ways.\n",
                        "\n",
                        "**Process:**\n",
                        "A process is an independent program in execution. It has its own memory space, stack, and resources. Processes are isolated from each other, meaning one process cannot directly access the memory of another.\n",
                        "\n",
                        "**Thread:**\n",
                        "A thread is a lightweight unit of execution within a process. Threads share the same memory space and resources of the parent process. This makes communication between threads faster but also requires careful synchronization.\n",
                        "\n",
                        "------------------------------\n",
                        "\n",
                        "[AFTER - Fine-Tuned Adapter]\n",
                        "In the context of Operating Systems, a Process is a heavyweight unit of execution that has its own separate memory address space, including text, data, and stack segments. Processes are isolated from one another.\n",
                        "\n",
                        "A Thread, often called a lightweight process, exists within a process. Threads share the same address space (code and data) but have their own stack and registers. Context switching between threads is faster than between processes because improved locality and less overhead are required to switch.\n"
                    ]
                }
            ],
            "source": [
                "print(\"=== COMPARISON ===\\n\")\n",
                "print(f\"PROMPT: {test_instruction}\\n\")\n",
                "\n",
                "print(\"[BEFORE - Ollama Base Model]\")\n",
                "print(baseline_response)\n",
                "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
                "\n",
                "print(\"[AFTER - Fine-Tuned Adapter]\")\n",
                "print(ft_response)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}